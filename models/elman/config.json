{
  "model_type": "custom-elman",
  "config_version": 1,
  "spec": {
    "vocab_size": 18,
    "image_shape": null,
    "layers": [
      {
        "type": "embedding",
        "num_embeddings": 18,
        "embedding_dim": 18,
        "kind": "one_hot",
        "_name": "embed"
      },
      {
        "type": "elman",
        "input_size": 18,
        "hidden_size": 16,
        "nonlinearity": "tanh",
        "batch_first": true,
        "_name": "elman"
      },
      {
        "type": "lm_head",
        "hidden_size": 16,
        "vocab_size": 18,
        "bias": true,
        "_name": "lm_head"
      }
    ]
  },
  "metadata": {
    "time_saved_unix": 1756960838,
    "python_version": "3.10.18",
    "torch_version": "2.8.0",
    "uses_safetensors": true,
    "notes": "Elman one-hot experiment"
  },
  "arch_hash": "f76d940b471a94f3aa4b55cc5e4f29a4a293866ae7d8030d94f774fb10392a06"
}