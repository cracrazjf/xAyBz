{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 720,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 1.6275348663330078,
      "learning_rate": 0.009875,
      "loss": 2.6716,
      "step": 10
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 1.7533951997756958,
      "learning_rate": 0.00973611111111111,
      "loss": 2.0502,
      "step": 20
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 2.1698484420776367,
      "learning_rate": 0.009597222222222222,
      "loss": 1.9998,
      "step": 30
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 2.6547789573669434,
      "learning_rate": 0.009458333333333332,
      "loss": 1.2902,
      "step": 40
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 2.7217400074005127,
      "learning_rate": 0.009319444444444444,
      "loss": 1.6851,
      "step": 50
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 2.6890709400177,
      "learning_rate": 0.009180555555555555,
      "loss": 1.7811,
      "step": 60
    },
    {
      "epoch": 0.9722222222222222,
      "grad_norm": 0.8939241170883179,
      "learning_rate": 0.009041666666666667,
      "loss": 1.1143,
      "step": 70
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 2.87846040725708,
      "learning_rate": 0.008902777777777777,
      "loss": 1.0711,
      "step": 80
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.5001086592674255,
      "learning_rate": 0.008763888888888889,
      "loss": 1.0479,
      "step": 90
    },
    {
      "epoch": 1.3888888888888888,
      "grad_norm": 2.9686832427978516,
      "learning_rate": 0.008625,
      "loss": 1.6218,
      "step": 100
    },
    {
      "epoch": 1.5277777777777777,
      "grad_norm": 0.662022054195404,
      "learning_rate": 0.008486111111111111,
      "loss": 1.0088,
      "step": 110
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 2.521768569946289,
      "learning_rate": 0.008347222222222223,
      "loss": 1.4823,
      "step": 120
    },
    {
      "epoch": 1.8055555555555556,
      "grad_norm": 0.6853688955307007,
      "learning_rate": 0.008208333333333333,
      "loss": 0.8935,
      "step": 130
    },
    {
      "epoch": 1.9444444444444444,
      "grad_norm": 2.2755603790283203,
      "learning_rate": 0.008069444444444445,
      "loss": 1.1626,
      "step": 140
    },
    {
      "epoch": 2.0833333333333335,
      "grad_norm": 2.0253219604492188,
      "learning_rate": 0.007930555555555555,
      "loss": 0.7685,
      "step": 150
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 1.9882736206054688,
      "learning_rate": 0.007791666666666667,
      "loss": 1.3085,
      "step": 160
    },
    {
      "epoch": 2.361111111111111,
      "grad_norm": 0.4327738285064697,
      "learning_rate": 0.0076527777777777774,
      "loss": 0.6047,
      "step": 170
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.28239646553993225,
      "learning_rate": 0.007513888888888889,
      "loss": 0.678,
      "step": 180
    },
    {
      "epoch": 2.638888888888889,
      "grad_norm": 2.0446648597717285,
      "learning_rate": 0.0073750000000000005,
      "loss": 0.5038,
      "step": 190
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 0.23312878608703613,
      "learning_rate": 0.007236111111111111,
      "loss": 0.5924,
      "step": 200
    },
    {
      "epoch": 2.9166666666666665,
      "grad_norm": 2.0666651725769043,
      "learning_rate": 0.007097222222222223,
      "loss": 1.0306,
      "step": 210
    },
    {
      "epoch": 3.0555555555555554,
      "grad_norm": 0.24579481780529022,
      "learning_rate": 0.006958333333333333,
      "loss": 0.7925,
      "step": 220
    },
    {
      "epoch": 3.1944444444444446,
      "grad_norm": 0.2411624938249588,
      "learning_rate": 0.006819444444444445,
      "loss": 0.7652,
      "step": 230
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.1995997577905655,
      "learning_rate": 0.006680555555555555,
      "loss": 0.6594,
      "step": 240
    },
    {
      "epoch": 3.4722222222222223,
      "grad_norm": 1.9481267929077148,
      "learning_rate": 0.006541666666666667,
      "loss": 0.5984,
      "step": 250
    },
    {
      "epoch": 3.611111111111111,
      "grad_norm": 2.220616102218628,
      "learning_rate": 0.006402777777777777,
      "loss": 0.651,
      "step": 260
    },
    {
      "epoch": 3.75,
      "grad_norm": 1.8132823705673218,
      "learning_rate": 0.006263888888888889,
      "loss": 0.8072,
      "step": 270
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 0.1387544721364975,
      "learning_rate": 0.006125,
      "loss": 0.5309,
      "step": 280
    },
    {
      "epoch": 4.027777777777778,
      "grad_norm": 0.09225335717201233,
      "learning_rate": 0.005986111111111111,
      "loss": 0.6335,
      "step": 290
    },
    {
      "epoch": 4.166666666666667,
      "grad_norm": 2.235480785369873,
      "learning_rate": 0.005847222222222222,
      "loss": 0.8718,
      "step": 300
    },
    {
      "epoch": 4.305555555555555,
      "grad_norm": 0.09268023073673248,
      "learning_rate": 0.0057083333333333335,
      "loss": 0.3347,
      "step": 310
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 2.2148830890655518,
      "learning_rate": 0.005569444444444445,
      "loss": 0.6583,
      "step": 320
    },
    {
      "epoch": 4.583333333333333,
      "grad_norm": 0.1006041020154953,
      "learning_rate": 0.005430555555555556,
      "loss": 0.5348,
      "step": 330
    },
    {
      "epoch": 4.722222222222222,
      "grad_norm": 1.6949944496154785,
      "learning_rate": 0.005291666666666667,
      "loss": 0.4695,
      "step": 340
    },
    {
      "epoch": 4.861111111111111,
      "grad_norm": 1.7990431785583496,
      "learning_rate": 0.005152777777777777,
      "loss": 1.0864,
      "step": 350
    },
    {
      "epoch": 5.0,
      "grad_norm": 2.0935416221618652,
      "learning_rate": 0.005013888888888889,
      "loss": 0.3995,
      "step": 360
    },
    {
      "epoch": 5.138888888888889,
      "grad_norm": 2.0473556518554688,
      "learning_rate": 0.004875,
      "loss": 0.7096,
      "step": 370
    },
    {
      "epoch": 5.277777777777778,
      "grad_norm": 1.9394659996032715,
      "learning_rate": 0.004736111111111111,
      "loss": 0.6991,
      "step": 380
    },
    {
      "epoch": 5.416666666666667,
      "grad_norm": 1.9965193271636963,
      "learning_rate": 0.004597222222222222,
      "loss": 0.5697,
      "step": 390
    },
    {
      "epoch": 5.555555555555555,
      "grad_norm": 1.8620270490646362,
      "learning_rate": 0.004458333333333334,
      "loss": 0.215,
      "step": 400
    },
    {
      "epoch": 5.694444444444445,
      "grad_norm": 0.055830687284469604,
      "learning_rate": 0.004319444444444444,
      "loss": 0.9562,
      "step": 410
    },
    {
      "epoch": 5.833333333333333,
      "grad_norm": 0.05920824781060219,
      "learning_rate": 0.004180555555555555,
      "loss": 0.5717,
      "step": 420
    },
    {
      "epoch": 5.972222222222222,
      "grad_norm": 2.133169412612915,
      "learning_rate": 0.0040416666666666665,
      "loss": 0.6517,
      "step": 430
    },
    {
      "epoch": 6.111111111111111,
      "grad_norm": 1.903356671333313,
      "learning_rate": 0.003902777777777778,
      "loss": 0.5764,
      "step": 440
    },
    {
      "epoch": 6.25,
      "grad_norm": 1.9825794696807861,
      "learning_rate": 0.003763888888888889,
      "loss": 0.5836,
      "step": 450
    },
    {
      "epoch": 6.388888888888889,
      "grad_norm": 2.029980182647705,
      "learning_rate": 0.0036249999999999998,
      "loss": 0.5685,
      "step": 460
    },
    {
      "epoch": 6.527777777777778,
      "grad_norm": 2.0770533084869385,
      "learning_rate": 0.003486111111111111,
      "loss": 0.677,
      "step": 470
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.04919483885169029,
      "learning_rate": 0.003347222222222222,
      "loss": 0.3539,
      "step": 480
    },
    {
      "epoch": 6.805555555555555,
      "grad_norm": 2.0732998847961426,
      "learning_rate": 0.003208333333333334,
      "loss": 0.65,
      "step": 490
    },
    {
      "epoch": 6.944444444444445,
      "grad_norm": 0.05444560945034027,
      "learning_rate": 0.0030694444444444445,
      "loss": 0.3527,
      "step": 500
    },
    {
      "epoch": 7.083333333333333,
      "grad_norm": 1.9480284452438354,
      "learning_rate": 0.0029305555555555556,
      "loss": 1.0524,
      "step": 510
    },
    {
      "epoch": 7.222222222222222,
      "grad_norm": 0.04453672841191292,
      "learning_rate": 0.0027916666666666667,
      "loss": 0.6746,
      "step": 520
    },
    {
      "epoch": 7.361111111111111,
      "grad_norm": 2.096057176589966,
      "learning_rate": 0.0026527777777777778,
      "loss": 0.3273,
      "step": 530
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.05042066052556038,
      "learning_rate": 0.002513888888888889,
      "loss": 0.3408,
      "step": 540
    },
    {
      "epoch": 7.638888888888889,
      "grad_norm": 0.04359591379761696,
      "learning_rate": 0.002375,
      "loss": 0.5158,
      "step": 550
    },
    {
      "epoch": 7.777777777777778,
      "grad_norm": 2.0669939517974854,
      "learning_rate": 0.0022361111111111115,
      "loss": 0.7083,
      "step": 560
    },
    {
      "epoch": 7.916666666666667,
      "grad_norm": 1.9850740432739258,
      "learning_rate": 0.002097222222222222,
      "loss": 0.4677,
      "step": 570
    },
    {
      "epoch": 8.055555555555555,
      "grad_norm": 2.0175578594207764,
      "learning_rate": 0.001958333333333333,
      "loss": 0.8035,
      "step": 580
    },
    {
      "epoch": 8.194444444444445,
      "grad_norm": 2.0363481044769287,
      "learning_rate": 0.0018194444444444445,
      "loss": 0.8043,
      "step": 590
    },
    {
      "epoch": 8.333333333333334,
      "grad_norm": 0.03969215229153633,
      "learning_rate": 0.0016805555555555558,
      "loss": 0.5555,
      "step": 600
    },
    {
      "epoch": 8.472222222222221,
      "grad_norm": 0.03902500122785568,
      "learning_rate": 0.0015416666666666669,
      "loss": 0.2072,
      "step": 610
    },
    {
      "epoch": 8.61111111111111,
      "grad_norm": 1.8083616495132446,
      "learning_rate": 0.0014027777777777777,
      "loss": 0.5952,
      "step": 620
    },
    {
      "epoch": 8.75,
      "grad_norm": 2.1395087242126465,
      "learning_rate": 0.0012638888888888888,
      "loss": 0.7233,
      "step": 630
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 0.033428970724344254,
      "learning_rate": 0.0011250000000000001,
      "loss": 0.3706,
      "step": 640
    },
    {
      "epoch": 9.027777777777779,
      "grad_norm": 2.026252269744873,
      "learning_rate": 0.000986111111111111,
      "loss": 0.9382,
      "step": 650
    },
    {
      "epoch": 9.166666666666666,
      "grad_norm": 0.03319769352674484,
      "learning_rate": 0.0008472222222222223,
      "loss": 0.665,
      "step": 660
    },
    {
      "epoch": 9.305555555555555,
      "grad_norm": 2.076711416244507,
      "learning_rate": 0.0007083333333333333,
      "loss": 0.3527,
      "step": 670
    },
    {
      "epoch": 9.444444444444445,
      "grad_norm": 1.9300432205200195,
      "learning_rate": 0.0005694444444444445,
      "loss": 0.679,
      "step": 680
    },
    {
      "epoch": 9.583333333333334,
      "grad_norm": 0.03280367702245712,
      "learning_rate": 0.00043055555555555555,
      "loss": 0.5657,
      "step": 690
    },
    {
      "epoch": 9.722222222222221,
      "grad_norm": 1.9011549949645996,
      "learning_rate": 0.0002916666666666667,
      "loss": 0.7771,
      "step": 700
    },
    {
      "epoch": 9.86111111111111,
      "grad_norm": 0.030104348435997963,
      "learning_rate": 0.00015277777777777777,
      "loss": 0.4657,
      "step": 710
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.03859792649745941,
      "learning_rate": 1.388888888888889e-05,
      "loss": 0.354,
      "step": 720
    }
  ],
  "logging_steps": 10,
  "max_steps": 720,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
